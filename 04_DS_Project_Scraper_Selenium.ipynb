{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Science SS 2022\n",
    "## Task 1 - Data Preparation and Modeling\n",
    "### Task 1a - Crawl and parse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading links from csv..\n",
      "Setting up chrome webdriver..\n",
      "Starting scraping data from pages..\n",
      "Processing scraping of page 1 of 100\n",
      "Processing scraping of page 2 of 100\n",
      "Processing scraping of page 3 of 100\n",
      "Processing scraping of page 4 of 100\n",
      "Processing scraping of page 5 of 100\n",
      "Processing scraping of page 6 of 100\n",
      "Processing scraping of page 7 of 100\n",
      "Processing scraping of page 8 of 100\n",
      "Processing scraping of page 9 of 100\n",
      "Processing scraping of page 10 of 100\n",
      "Processing scraping of page 11 of 100\n",
      "Processing scraping of page 12 of 100\n",
      "Processing scraping of page 13 of 100\n",
      "Processing scraping of page 14 of 100\n",
      "Processing scraping of page 15 of 100\n",
      "Processing scraping of page 16 of 100\n",
      "Processing scraping of page 17 of 100\n",
      "Processing scraping of page 18 of 100\n",
      "Processing scraping of page 19 of 100\n",
      "Processing scraping of page 20 of 100\n",
      "Processing scraping of page 21 of 100\n",
      "Processing scraping of page 22 of 100\n",
      "Processing scraping of page 23 of 100\n",
      "Processing scraping of page 24 of 100\n",
      "Processing scraping of page 25 of 100\n",
      "Processing scraping of page 26 of 100\n",
      "Processing scraping of page 27 of 100\n",
      "Processing scraping of page 28 of 100\n",
      "Processing scraping of page 29 of 100\n",
      "Processing scraping of page 30 of 100\n",
      "Processing scraping of page 31 of 100\n",
      "Processing scraping of page 32 of 100\n",
      "Processing scraping of page 33 of 100\n",
      "Processing scraping of page 34 of 100\n",
      "Processing scraping of page 35 of 100\n",
      "Processing scraping of page 36 of 100\n",
      "Processing scraping of page 37 of 100\n",
      "Processing scraping of page 38 of 100\n",
      "Processing scraping of page 39 of 100\n",
      "Processing scraping of page 40 of 100\n",
      "Processing scraping of page 41 of 100\n",
      "Processing scraping of page 42 of 100\n",
      "Processing scraping of page 43 of 100\n",
      "Processing scraping of page 44 of 100\n",
      "Processing scraping of page 45 of 100\n",
      "Processing scraping of page 46 of 100\n",
      "Processing scraping of page 47 of 100\n",
      "Processing scraping of page 48 of 100\n",
      "Processing scraping of page 49 of 100\n",
      "Processing scraping of page 50 of 100\n",
      "Processing scraping of page 51 of 100\n",
      "Processing scraping of page 52 of 100\n",
      "Processing scraping of page 53 of 100\n",
      "Processing scraping of page 54 of 100\n",
      "Processing scraping of page 55 of 100\n",
      "Processing scraping of page 56 of 100\n",
      "Processing scraping of page 57 of 100\n",
      "Processing scraping of page 58 of 100\n",
      "Processing scraping of page 59 of 100\n",
      "Processing scraping of page 60 of 100\n",
      "Processing scraping of page 61 of 100\n",
      "Processing scraping of page 62 of 100\n",
      "Processing scraping of page 63 of 100\n",
      "Processing scraping of page 64 of 100\n",
      "Processing scraping of page 65 of 100\n",
      "Processing scraping of page 66 of 100\n",
      "Processing scraping of page 67 of 100\n",
      "Processing scraping of page 68 of 100\n",
      "Processing scraping of page 69 of 100\n",
      "Processing scraping of page 70 of 100\n",
      "Processing scraping of page 71 of 100\n",
      "Processing scraping of page 72 of 100\n",
      "Processing scraping of page 73 of 100\n",
      "Processing scraping of page 74 of 100\n",
      "Processing scraping of page 75 of 100\n",
      "Processing scraping of page 76 of 100\n",
      "Processing scraping of page 77 of 100\n",
      "Processing scraping of page 78 of 100\n",
      "Processing scraping of page 79 of 100\n",
      "Processing scraping of page 80 of 100\n",
      "Processing scraping of page 81 of 100\n",
      "Processing scraping of page 82 of 100\n",
      "Processing scraping of page 83 of 100\n",
      "Processing scraping of page 84 of 100\n",
      "Processing scraping of page 85 of 100\n",
      "Processing scraping of page 86 of 100\n",
      "Processing scraping of page 87 of 100\n",
      "Processing scraping of page 88 of 100\n",
      "Processing scraping of page 89 of 100\n",
      "Processing scraping of page 90 of 100\n",
      "Processing scraping of page 91 of 100\n",
      "Processing scraping of page 92 of 100\n",
      "Processing scraping of page 93 of 100\n",
      "Processing scraping of page 94 of 100\n",
      "Processing scraping of page 95 of 100\n",
      "Processing scraping of page 96 of 100\n",
      "Processing scraping of page 97 of 100\n",
      "Processing scraping of page 98 of 100\n",
      "Processing scraping of page 99 of 100\n",
      "Processing scraping of page 100 of 100\n",
      "Exporting scraped data to csv\n",
      "Finished scraping! Took 4 min to scrape 2\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Function for read in csv with web-links to scrape\n",
    "def read_links(source):\n",
    "    datatable = pd.read_csv(source)\n",
    "    raw_links = datatable['roughguide link'].tolist()\n",
    "    return raw_links\n",
    "\n",
    "\n",
    "# Function for skipping cookie banner\n",
    "def skip_cookie():\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"cf2Lf6\")))\n",
    "    driver.find_element(By.CLASS_NAME, 'cf2Lf6').click()\n",
    "    print('>> Cookie-Banner successful skipped!')\n",
    "\n",
    "\n",
    "# Function for extracting place name of webpage\n",
    "def get_place():\n",
    "    raw_place = driver.find_element(By.XPATH, '/html/body/div[1]/div/div/div[4]/div/section[1]/div/div/main/div/section/h1').text\n",
    "    return raw_place\n",
    "\n",
    "\n",
    "# Function for extracting place description of webpage\n",
    "def get_content():\n",
    "    content_text = ''\n",
    "    content_element = driver.find_element(By.CLASS_NAME, 'DestinationPageContent')\n",
    "    raw_text = content_element.find_elements(By.TAG_NAME, 'p')\n",
    "    for paragraph in raw_text:\n",
    "        content_text += str(' ' + paragraph.text)\n",
    "    content_text = ' '.join(content_text.splitlines())\n",
    "    content_text = content_text.replace(';', ' -')\n",
    "    content_text = content_text.replace(\"In-depth, easy-to-use travel guides filled with expert advice.\", '')\n",
    "    content_text = content_text.replace(\"Use Rough Guides' trusted partners for great rates\", '')\n",
    "    content_text = content_text.strip()\n",
    "    return content_text\n",
    "\n",
    "\n",
    "# Function for saving scraped data to csv\n",
    "def save_to_csv(lst, destination_file_path):\n",
    "    data = pd.DataFrame(lst, columns=['Link', 'Place', 'Content'])\n",
    "    data.to_csv(destination_file_path, header=['Link', 'Place', 'Content'], encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "\n",
    "#### MAIN ####\n",
    "\n",
    "# Set variables\n",
    "source_path = 'C:/Users/Henryy/Downloads/DataScience2022_RoughGuides.csv'       ## Set ##\n",
    "destination_file_path = 'C:/Users/Henryy/Downloads/ds_test.csv'                 ## Set ##\n",
    "count = 1\n",
    "lst = []\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "\n",
    "    # Read in csv with page links\n",
    "    print('Reading links from csv..')\n",
    "    links = read_links(source_path)\n",
    "\n",
    "    # Start driver\n",
    "    print('Setting up chrome webdriver..')\n",
    "    driver = webdriver.Chrome()\n",
    "    WebDriverWait(driver, 10)\n",
    "\n",
    "    # Iterate over links\n",
    "    print('Starting scraping data from pages..')\n",
    "    for link in links:\n",
    "\n",
    "        # Open page\n",
    "        print('Processing scraping of page {0} of {1}'.format(count, len(links)))\n",
    "        driver.get(link)\n",
    "\n",
    "        # Check if first run, if yes then skip cookie banner\n",
    "        if count == 0:\n",
    "            print('>> Cookie-Banner must be skipped!')\n",
    "            skip_cookie()\n",
    "\n",
    "        # Get header text (place name) of page\n",
    "        place = get_place()\n",
    "\n",
    "        # Get content text (place description) of page\n",
    "        content = get_content()\n",
    "\n",
    "        # Append place and content to list\n",
    "        lst.append([link, place, content])\n",
    "\n",
    "        # Wait 1 sec and repeat procedure for next page\n",
    "        time.sleep(1)\n",
    "        count += 1\n",
    "\n",
    "    # Save list as dataframe and export to csv\n",
    "    print('Exporting scraped data to csv')\n",
    "    save_to_csv(lst, destination_file_path)\n",
    "\n",
    "    # Exit driver\n",
    "    print('Finished scraping! Took %s min to scrape' % round(((time.time() - start_time)/60)), 2)\n",
    "    time.sleep(2)\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "\n",
    "    # Catch Keyboard Interrupt\n",
    "    print('The programm was interrupted by keyboard')\n",
    "    exit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}