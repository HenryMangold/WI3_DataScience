{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Science SS 2022\n",
    "## Task 1 - Data Preparation and Modeling\n",
    "### Task 1a - Crawl and parse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'html' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\TIM_ST~1\\AppData\\Local\\Temp/ipykernel_4152/1755385229.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[0mraw_place\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdriver\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind_element\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mXPATH\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'/html/body/div[1]/div/div/div[4]/div/section[1]/div/div/main/div/section/h1'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mraw_place\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 19\u001B[1;33m \u001B[0mhtml\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mbody\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdiv\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdiv\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdiv\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdiv\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdiv\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mmain\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdiv\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdiv\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdiv\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdiv\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdiv\u001B[0m\u001B[1;33m/\u001B[0m\u001B[0mdiv\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mget_place_h2\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtitle_id\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'html' is not defined"
     ]
    }
   ],
   "source": [
    "# Function for read in csv with web-links to scrape\n",
    "def read_links(source):\n",
    "    datatable = pd.read_csv(source)\n",
    "    raw_links = datatable['roughguide link'].tolist()\n",
    "    return raw_links\n",
    "\n",
    "\n",
    "# Function for skipping cookie banner\n",
    "def skip_cookie():\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"cf2Lf6\")))\n",
    "    driver.find_element(By.CLASS_NAME, 'cf2Lf6').click()\n",
    "    print('>> Cookie-Banner successful skipped!')\n",
    "\n",
    "\n",
    "# Function for extracting place name of webpage\n",
    "def get_place():\n",
    "    raw_place = driver.find_element(By.XPATH, '/html/body/div[1]/div/div/div[4]/div/section[1]/div/div/main/div/section/h1').text\n",
    "    return raw_place\n",
    "\n",
    "\n",
    "def get_place_h2(title_id):\n",
    "    raw_place = driver.find_element(By.ID, title_id).text\n",
    "    return raw_place\n",
    "\n",
    "# Function for extracting place description of webpage\n",
    "def get_full_content():\n",
    "    content_text = ''\n",
    "    content_element = driver.find_element(By.CLASS_NAME, 'DestinationPageContent')\n",
    "    raw_text = content_element.find_elements(By.TAG_NAME, 'p')\n",
    "    for paragraph in raw_text:\n",
    "        content_text += str(' ' + paragraph.text)\n",
    "    return content_text\n",
    "\n",
    "\n",
    "def get_content_between_headlines(title_id):\n",
    "    # h2 element with id that contains title (all lower case and - instead of ' ')\n",
    "    keep_text = False\n",
    "    content_text = ''\n",
    "    xpath = '//*[@id=\"{0}\"]'.format(title_id)\n",
    "    container = driver.find_element(By.XPATH, '/html/body/div[1]/div/div/div[4]/div/main/div[2]/div/div/div/div/div[3]')\n",
    "    #start_element = driver.find_element(By.XPATH, xpath)\n",
    "    #current_element = start_element.find_element(By.XPATH, '//following-sibling::*')\n",
    "    # for element in container:\n",
    "    #     if element.tag_name == 'h2':\n",
    "    #         # break while loop if next element is a h2\n",
    "    #         break\n",
    "    #     content_text += str(' ' + current_element.text)\n",
    "    #     # continue to next element\n",
    "    #     current_element = current_element.find_element(By.XPATH, '//following-sibling::*')\n",
    "\n",
    "    for element in container:\n",
    "        if element.getAttribute(\"id\") == title_id:\n",
    "            keep_text = True\n",
    "        tag_value= element.get_attribute('outerHTML ').split('',1)[0]  # gets html tag of current element\n",
    "        if keep_text and tag_value != 'h2':\n",
    "            content_text += str(' ' + element.text)\n",
    "    return content_text\n",
    "\n",
    "\n",
    "def clean_content_text(content_text):\n",
    "    content_text = ' '.join(content_text.splitlines())\n",
    "    content_text = content_text.replace(';', ' -')\n",
    "    content_text = content_text.replace(\"In-depth, easy-to-use travel guides filled with expert advice.\", '')\n",
    "    content_text = content_text.replace(\"Use Rough Guides' trusted partners for great rates\", '')\n",
    "    content_text = content_text.strip()\n",
    "    return content_text\n",
    "\n",
    "\n",
    "# Function for saving scraped data to csv\n",
    "def save_to_csv(lst, destination_file_path):\n",
    "    data = pd.DataFrame(lst, columns=['Link', 'Place', 'Content'])\n",
    "    data.to_csv(destination_file_path, header=['Link', 'Place', 'Content'], encoding='utf-8-sig')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Set variables\n",
    "source_path = './data/link_test.csv'       ## Set ##\n",
    "destination_file_path = './data/ds_test.csv'                 ## Set ##\n",
    "\n",
    "count = 1\n",
    "lst = []\n",
    "start_time = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading links from csv..\n",
      "Setting up chrome webdriver..\n",
      "Starting scraping data from pages..\n",
      "Processing scraping of page 2 of 3\n",
      "Processing scraping of page 3 of 3\n",
      "link to h2:\n",
      "The programm was interrupted by keyboard\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Read in csv with page links\n",
    "    print('Reading links from csv..')\n",
    "    links = read_links(source_path)\n",
    "\n",
    "    # Start driver\n",
    "    print('Setting up chrome webdriver..')\n",
    "    driver = webdriver.Chrome()\n",
    "    WebDriverWait(driver, 10)\n",
    "\n",
    "    # Iterate over links\n",
    "    print('Starting scraping data from pages..')\n",
    "    for link in links:\n",
    "\n",
    "        # Open page\n",
    "        print('Processing scraping of page {0} of {1}'.format(count, len(links)))\n",
    "        driver.get(link)\n",
    "\n",
    "        # Check if first run, if yes then skip cookie banner\n",
    "        # if count == 0:\n",
    "        #     print('>> Cookie-Banner must be skipped!')\n",
    "        #     skip_cookie()\n",
    "\n",
    "        if '#' in link:\n",
    "            print('link to h2:')\n",
    "            # This link only refers to a part of the page.\n",
    "            # Only content between the h2 Headline and the next h2 should be extracted\n",
    "            title_id = link.split('#')[1]\n",
    "\n",
    "            # get place name\n",
    "            place = get_place_h2(title_id)\n",
    "\n",
    "            # get content until next h2\n",
    "            content_raw = get_content_between_headlines(title_id)\n",
    "            content = clean_content_text(content_raw)\n",
    "\n",
    "            print(place)\n",
    "            print(content)\n",
    "\n",
    "        else:\n",
    "            # whole page refers to the place and should be extracted\n",
    "            # Get header text (place name) of page\n",
    "            place = get_place()\n",
    "\n",
    "            # Get content text (place description) of page\n",
    "            content_raw = get_full_content()\n",
    "            content = clean_content_text(content_raw)\n",
    "\n",
    "        # Append place and content to list\n",
    "        lst.append([link, place, content])\n",
    "\n",
    "        # Wait 1 sec and repeat procedure for next page\n",
    "        time.sleep(1)\n",
    "        count += 1\n",
    "\n",
    "    # Save list as dataframe and export to csv\n",
    "    print('Exporting scraped data to csv')\n",
    "    save_to_csv(lst, destination_file_path)\n",
    "\n",
    "    # Exit driver\n",
    "    print('Finished scraping! Took %s min to scrape' % round(((time.time() - start_time)/60)), 2)\n",
    "    time.sleep(2)\n",
    "    driver.quit()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Catch Keyboard Interrupt\n",
    "    print('The programm was interrupted by keyboard')\n",
    "    exit()\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # try saving current process in dataframe\n",
    "    print('trying to Export scraped data to csv')\n",
    "    save_to_csv(lst, destination_file_path)\n",
    "    print('done')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}